policy_type: BCFoundationDmpelPolicy
extra_num_layers: 0
extra_hidden_size: 256
embed_size: 768

transformer_input_size: null
transformer_num_layers: 6
transformer_num_heads: 8
transformer_head_output_size: 96
transformer_mlp_hidden_size: 1024
transformer_dropout: 0.15
transformer_max_seq_len: 10
film_hidden_size: 256

task_emb_size: 64
task_emb_net_hidden_size: 256

use_lora: 'MoELoRA' # 'LoRAqkv' or 'None'
s_lora_image: true
s_lora_image_layer_list: [6,7,8,9,10,11]
s_lora_text: true
s_lora_text_layer_list: [6,7,8,9,10,11]
t_lora: true
t_lora_layer_list: 'all'
fullft: false
lora_dropout: 0.15
merge_AB: 'weight' # 'output' or 'weight'
tune_bias: true

init_pool_size: 0
moe_topk: 3
query_use_mean_img: true
query_use_diff_img: false
query_use_proprio: true
ll_expert_init: 'ortho' # 'random' or 'prev_mean'
ll_expert_per_task: 1
has_pretrained: true
router_type: 'coeff' # 'emb' or 'coeff'
router_coeff_seperate: true
router_lr_scale: 1.0
infer_interval: 1

defaults:
    - data_augmentation@color_aug: batch_wise_img_color_jitter_group_aug.yaml
    - data_augmentation@translation_aug: translation_aug.yaml
    - data_augmentation@affine_aug: affine_aug.yaml
    - image_encoder: clip_image_encoder_frozen.yaml
    - language_encoder: clip_text_encoder_frozen.yaml
    - position_encoding@temporal_position_encoding: sinusoidal_position_encoding.yaml
    - policy_head: gmm_head.yaml
    - adapter: lora16.yaml
